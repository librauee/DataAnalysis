# -*- coding: utf-8 -*-
"""
Created on Wed Feb 13 15:55:47 2019

@author: Administrator
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pandas.io.json import json_normalize
from pymongo import MongoClient
from pyecharts import Bar,Line,Overlap,Page,WordCloud,Pie,Funnel
import jieba
from collections import Counter


page=Page()   #å®ä¾‹åŒ–Page
conn = MongoClient(host='127.0.0.1', port=27017)  # å®ä¾‹åŒ–MongoClient
db = conn.get_database('maoyan')  # è¿æ¥åˆ°maoyanæ•°æ®åº“

maoyan = db.get_collection('maoyan') # è¿æ¥åˆ°é›†åˆmaoyan
mon_data = maoyan.find()  # æŸ¥è¯¢è¿™ä¸ªé›†åˆä¸‹çš„æ‰€æœ‰è®°å½•

data=json_normalize([comment for comment in mon_data])

data = data.drop(columns='_id')
data = data.drop_duplicates(subset='userId')
data['time'] = pd.to_datetime(data['time']/1000, unit='s')
#è¯»å…¥çš„æ—¶é—´æ•°æ®æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œè½¬æ¢æˆdatetimeæ ¼å¼,å•ä½åŒ–ä¸ºç§’
data = data[data['time']>=pd.to_datetime('2019-02-05 00:00:00')]
data.set_index(data['time'], inplace=True)

data.info()
print(data.head())

# æ•°æ®æ¸…æ´—


'''
dfä¿¡æ¯

Data columns (total 10 columns):
content       84114 non-null object
gender        84114 non-null int64
id            84114 non-null int64
nick          84114 non-null object
replyCount    84114 non-null int64
score         84114 non-null int64
time          84114 non-null datetime64[ns]
upCount       84114 non-null int64
userId        84114 non-null int64
userLevel     84114 non-null int64
dtypes: datetime64[ns](1), int64(7), object(2)
memory usage: 7.1+ MB

è¾“å‡ºå¤´éƒ¨ä¿¡æ¯

                              content  gender          id           nick  \
time                                                                       
2019-02-13 08:49:00      ä¸å¥½çœ‹ æ²¡æ„æ€ ç‰¹æ•ˆè¿˜è¡Œ       2  1057160823          çº¯æ´å°èŒèŒ   
2019-02-13 08:49:00          åœ°æœ‰ç‚¹ç²˜ï¼Œå“ˆå“ˆå“ˆ       1  1057162283          L1uä¸‰åºŸ   
2019-02-13 08:49:00  ç‰¹æ•ˆçˆ†ç‚¸ï¼  æ°¸è¿œæ”¯æŒå›½äº§ç”µå½±ï¼       0  1057159336       å…«ç™¾é€—æ¯”å¥”åŒ—å¡ğŸ˜¶   
2019-02-13 08:49:00       å¾ˆæ³ªç›®å•Šã€‚ å ªæ¯”å¥½è±å       2  1057160811  Kimi864293104   
2019-02-13 08:49:00            å¥½çœ‹ï¼ï¼ï¼ï¼       2  1057156983             éšä¾¿   

                     replyCount  score                time  upCount  \
time                                                                  
2019-02-13 08:49:00           0      5 2019-02-13 08:49:00        0   
2019-02-13 08:49:00           0     10 2019-02-13 08:49:00        0   
2019-02-13 08:49:00           0     10 2019-02-13 08:49:00        0   
2019-02-13 08:49:00           0     10 2019-02-13 08:49:00        0   
2019-02-13 08:49:00           0     10 2019-02-13 08:49:00        0   

                         userId  userLevel  
time                                        
2019-02-13 08:49:00    29444203          2  
2019-02-13 08:49:00   560675146          3  
2019-02-13 08:49:00  1022833680          2  
2019-02-13 08:49:00   170658854          1  
2019-02-13 08:49:00  1056871017          2  


'''


'''
æ€»ä½“è¯„ä»·
'''
print(data['score'].mean())
score_total = data['score'].value_counts().sort_index()
#sort_index(ascending=True) æ–¹æ³•å¯ä»¥å¯¹ç´¢å¼•è¿›è¡Œæ’åºæ“ä½œ
print(score_total)
bar = Bar("ã€Šæµæµªåœ°çƒã€‹å„è¯„åˆ†æ•°é‡", width=700)
line = Line("", width=700)
bar.add("", score_total.index, score_total.values, is_stack=True, is_label_show=True,
       bar_category_gap='40%', label_color = ['#196845'],
       legend_text_size=18,xaxis_label_textsize=18,yaxis_label_textsize=18)
line.add("", score_total.index, score_total.values+1000, is_smooth=True)

overlap = Overlap(width=700)
overlap.add(bar)
overlap.add(line)
#overlap.render()
page.add(overlap)

# ä½åˆ†ç™¾åˆ†æ¯”
low_score=score_total[:4].sum()/score_total.sum()*100
# é«˜åˆ†ç™¾åˆ†æ¯”
high_score=score_total[8:].sum()/score_total.sum()*100
# æ»¡åˆ†ç™¾åˆ†æ¯”
full_score=score_total[10:].sum()/score_total.sum()*100

print(u'ä½åˆ†å ç™¾åˆ†æ¯”ä¸º:{:.3f}%'.format(low_score))
print(u'é«˜åˆ†å ç™¾åˆ†æ¯”ä¸º:{:.3f}%'.format(high_score))
print(u'æ»¡åˆ†å ç™¾åˆ†æ¯”ä¸º:{:.3f}%'.format(full_score))
'''
ä½åˆ†å ç™¾åˆ†æ¯”ä¸º:3.419%
é«˜åˆ†å ç™¾åˆ†æ¯”ä¸º:90.625%
æ»¡åˆ†å ç™¾åˆ†æ¯”ä¸º:70.530%

'''


'''
æ€»ä½“è¯„ä»·çš„æ—¶é—´èµ°å‘
'''
score_by_time = data['score'].resample('H').mean()
#print(score_by_time)
line1 = Line("ã€Šæµæµªåœ°çƒã€‹å¹³å‡è¯„åˆ†æ—¶é—´èµ°å‘", width=700)
line1.add("", score_by_time.index.date, score_by_time.values, is_smooth=True,
         legend_text_size=18,xaxis_label_textsize=18,yaxis_label_textsize=18,
         xaxis_rotate=20, yaxis_min=8)
#line.render()
page.add(line1)



#è¾“å‡ºæœ€é«˜å’Œæœ€ä½å¹³å‡è¯„åˆ†çš„æ—¶é—´æ®µ
print(score_by_time.nsmallest(6))
print(score_by_time.nlargest(6))

'''
é«˜åˆ†çš„è¯„ä»·ç†ç”±
'''


jieba.add_word('å±ˆæ¥šè§')
jieba.add_word('åˆ˜å¯')
jieba.add_word('å´äº¬')
jieba.add_word('åˆ˜åŸ¹å¼º')
jieba.add_word('æå…‰æ´')
jieba.add_word('ç‹ç£Š')
jieba.add_word('å´å­Ÿè¾¾')
jieba.add_word('è¾¾å”')
jieba.add_word('éŸ©å­æ˜‚')
jieba.add_word('èµµä»Šéº¦')
jieba.add_word('éŸ©æœµæœµ')

swords = [x.strip() for x in open ('stopwords.txt')]

def plot_word_cloud1(data, swords):
    text = ''.join(data['content'])
    words = list(jieba.cut(text))
    ex_sw_words = []
    for word in words:
        if len(word)>1 and (word not in swords):
            ex_sw_words.append(word)
    c = Counter()
    c = Counter(ex_sw_words)
    wc_data = pd.DataFrame({'word':list(c.keys()), 'counts':list(c.values())}).sort_values(by='counts', ascending=False).head(100)
    wordcloud = WordCloud(width=1300, height=620)
    wordcloud.add("", wc_data['word'], wc_data['counts'], word_size_range=[20, 100])
    page.add(wordcloud)

# é«˜åˆ†çš„è¯„ä»·è¯äº‘

plot_word_cloud1(data=data[data['score']>7], swords=swords)
#é«˜åˆ†è¯„ä»·ç‚¹èµæ•°ç›®æœ€å¤šçš„è¯„ä»·

up_top_ten=data[data['score']>7].nlargest(10, 'upCount')
print("é«˜åˆ†è¯„ä»·ç‚¹èµæ•°ç›®æœ€å¤šçš„è¯„ä»·å¦‚ä¸‹ï¼š")
for i in up_top_ten['content']:
    print(i+'\n')
reply_top_ten=data[data['score']>7].nlargest(10, 'replyCount')    
print("é«˜åˆ†è¯„ä»·å›å¤æ•°ç›®æœ€å¤šçš„è¯„ä»·å¦‚ä¸‹ï¼š")
for i in reply_top_ten['content']:
    print(i+'\n')
    
'''
ä½åˆ†çš„ç†ç”±
'''

def plot_word_cloud2(data, swords):
    text = ''.join(data['content'])
    words = list(jieba.cut(text))
    ex_sw_words = []
    for word in words:
        if len(word)>1 and (word not in swords):
            ex_sw_words.append(word)
    c = Counter()
    c = Counter(ex_sw_words)
    wc_data = pd.DataFrame({'word':list(c.keys()), 'counts':list(c.values())}).sort_values(by='counts', ascending=False).head(100)
    wordcloud = WordCloud(width=1300, height=620)
    wordcloud.add("", wc_data['word'], wc_data['counts'], word_size_range=[20, 100])
    page.add(wordcloud)
# ä½åˆ†çš„è¯„ä»·
plot_word_cloud2(data=data[data['score']<4], swords=swords)
up_bottom_ten=data[data['score']<4].nlargest(10, 'upCount')
#ä½åˆ†è¯„ä»·ç‚¹èµæ•°ç›®æœ€å¤šçš„è¯„ä»·
print("ä½åˆ†è¯„ä»·ç‚¹èµæ•°ç›®æœ€å¤šçš„è¯„ä»·å¦‚ä¸‹ï¼š")
for i in up_bottom_ten['content']:
    print(i+'\n')
    
reply_bottom_ten=data[data['score']>7].nlargest(10, 'replyCount')    
print("ä½åˆ†è¯„ä»·å›å¤æ•°ç›®æœ€å¤šçš„è¯„ä»·å¦‚ä¸‹ï¼š")
for i in reply_bottom_ten['content']:
    print(i+'\n')   
    
'''    
ä½åˆ†çš„äººç¾¤æœ‰å“ªäº›ç‰¹å¾
'''

# æ€»ä½“çš„æ€§åˆ«æ¯”ä¾‹
#0,1,2åˆ†åˆ«æŒ‡æ€§åˆ«æœªçŸ¥ã€ç”·ã€å¥³
gender_total = data['gender'].value_counts()
pie1 = Pie("ã€Šæµæµªåœ°çƒã€‹è§‚ä¼—æ€§åˆ«æ¯”ä¾‹", width=700)
pie1.add("", ['æœªçŸ¥', 'ç”·', 'å¥³'], gender_total.values, is_stack=True, is_label_show=True,
       bar_category_gap='60%', label_color = ['#278f9d'])
#pie1.render()
page.add(pie1)

total_gender_percent=gender_total/gender_total.sum()*100

print('æ€»ä½“å„æ€§åˆ«å ç™¾åˆ†æ¯”ä¸º:')
print(total_gender_percent)

# ä½åˆ†çš„æ€§åˆ«æ¯”ä¾‹
gender_low = data.loc[data['score']<5, 'gender'].value_counts()
bar2 = Bar("ã€Šæµæµªåœ°çƒã€‹ä½åˆ†è¯„è®ºè§‚ä¼—æ€§åˆ«", width=700)
bar2.add("", ['æœªçŸ¥', 'ç”·', 'å¥³'], gender_low.values, is_stack=True, is_label_show=True,
       bar_category_gap='60%', label_color = ['#278f9d'],
       legend_text_size=18,xaxis_label_textsize=18,yaxis_label_textsize=18)
#bar.render()
page.add(bar2)
    
low_gender_percent=gender_low/gender_low.sum()*100
print('ä½åˆ†è§‚ä¼—å„æ€§åˆ«å ç™¾åˆ†æ¯”ä¸º:')
print(low_gender_percent)


# æ€»ä½“çš„ç­‰çº§æ¯”ä¾‹
level_total = data['userLevel'].value_counts().sort_index()
bar3 = Bar("ã€Šæµæµªåœ°çƒã€‹è§‚ä¼—ç­‰çº§", width=700)
bar3.add("", level_total.index, level_total.values, is_stack=True, is_label_show=True,
       bar_category_gap='40%', label_color = ['#130f40'],
       legend_text_size=18,xaxis_label_textsize=18,yaxis_label_textsize=18)

#bar.render()
page.add(bar3)
 # ä½åˆ†è¯„è®ºçš„è§‚ä¼—ç­‰çº§æ¯”ä¾‹
level_low = data.loc[data['score']<5, 'userLevel'].value_counts().sort_index()
bar4 = Bar("ã€Šæµæµªåœ°çƒã€‹ä½åˆ†è¯„è®ºçš„è§‚ä¼—ç­‰çº§", width=700)
bar4.add("", level_low.index, level_low.values, is_stack=True, is_label_show=True,
       bar_category_gap='40%', label_color = ['#130f40'],
       legend_text_size=18,xaxis_label_textsize=18,yaxis_label_textsize=18)   
    
    
#bar.render()
page.add(bar4)   

'''
é«˜ä½åˆ†å’Œå“ªä½æ¼”å‘˜æœ‰å…³

'''

mapping = {'liucixin':'åˆ˜æ…ˆæ¬£|å¤§åˆ˜', 'guofan':'éƒ­å¸†|éƒ­å¯¼', 'quchuxiao':'å±ˆæ¥šè§|åˆ˜å¯|æˆ·å£', 'wujing':'å´äº¬|åˆ˜åŸ¹å¼º', 
           'liguangjie':'æå…‰æ´|ç‹ç£Š', 'wumengda':'å´å­Ÿè¾¾|è¾¾å”|éŸ©å­æ˜‚', 'zhaojinmai':'èµµä»Šéº¦|éŸ©æœµæœµ'}
for key, value in mapping.items():
    data[key] = data['content'].str.contains(value)

# æ€»ä½“æåŠæ¬¡æ•°
    
staff_count = pd.Series({key: data.loc[data[key], 'score'].count() for key in mapping.keys()}).sort_values()
print(staff_count)
funnel = Funnel("ã€Šæµæµªåœ°çƒã€‹æ¼”èŒå‘˜æ€»ä½“æåŠæ¬¡æ•°", width=700)
funnel.add("", ['æå…‰æ´','éƒ­å¸†','èµµä»Šéº¦','å´å­Ÿè¾¾','å±ˆæ¥šè§','åˆ˜æ…ˆæ¬£','å´äº¬'], staff_count.values, is_stack=True, is_label_show=True,
      legend_pos="50%",label_color = ['#677fge'],legend_text_size=8)
funnel.render()
    
page.add(funnel)



average_score = pd.Series({key: data.loc[data[key], 'score'].mean() for key in mapping.keys()}).sort_values()
print(average_score)

bar6 = Bar("ã€Šæµæµªåœ°çƒã€‹æ¼”èŒå‘˜å¹³å‡åˆ†", width=700)
bar6.add("", ['èµµä»Šéº¦','å´å­Ÿè¾¾','å±ˆæ¥šè§','å´äº¬','æå…‰æ´','åˆ˜æ…ˆæ¬£','éƒ­å¸†'], np.round(average_score.values,2), is_stack=True, is_label_show=True,
       bar_category_gap='60%', label_color = ['#677fge'],
       legend_text_size=18,xaxis_label_textsize=18,yaxis_label_textsize=18)

#bar.render()
    
page.add(bar6)



staff_count_low = pd.Series({key: data.loc[data[key]&(data['score']<5), 'score'].count() for key in mapping.keys()}).sort_values()
print(staff_count_low)
staff_count_pct = np.round(staff_count_low/staff_count*100, 2).sort_values()
print(staff_count_pct)

bar7 = Bar("ã€Šæµæµªåœ°çƒã€‹æ¼”èŒå‘˜ä½åˆ†è¯„è®ºæåŠç™¾åˆ†æ¯”", width=700)
bar7.add("", ['éƒ­å¸†','åˆ˜æ…ˆæ¬£','æå…‰æ´','å±ˆæ¥šè§','èµµä»Šéº¦','å´äº¬','å´å­Ÿè¾¾'], staff_count_pct.values, is_stack=True, is_label_show=True,
       bar_category_gap='60%', label_color = ['#8gfdaf'],
       legend_text_size=18,xaxis_label_textsize=18,yaxis_label_textsize=18)

#bar.render()
    
page.add(bar7)
data[data['wumengda']&(data['score']<5)].nlargest(5, 'upCount')
for i in data[data['wumengda']&(data['score']<5)].nlargest(5, 'upCount')['content']:
    print(i+'\n')
    
data[data['wujing']&(data['score']<5)].nlargest(5, 'upCount')
for i in data[data['wujing']&(data['score']<5)].nlargest(5, 'upCount')['content']:
    print(i+'\n')
data[data['zhaojinmai']&(data['score']<5)].nlargest(5, 'upCount')
for i in data[data['zhaojinmai']&(data['score']<5)].nlargest(5, 'upCount')['content']:
    print(i+'\n')



'''
è¢«è¯„è®ºæœ€å¤šreplycountï¼Œç‚¹èµæœ€å¤šçš„upcount
'''


page.render("./commentAnalysis.html")









